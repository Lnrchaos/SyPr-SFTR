import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Union
from .symbolic import SymbolicLayer
from .probabilistic import ProbabilisticLayer
from .self_supervised import SelfSupervisedHead

class SymbolicTransformer(nn.Module):
    """
    Symbolic-Probabilistic-SelfSupervised-Federated Learning Transformer
    Combines symbolic reasoning, probabilistic modeling, and self-supervision
    in a federated learning compatible architecture.
    """
    def __init__(
        self,
        vocab_size: int = 30000,
        max_seq_length: int = 512,
        hidden_dim: int = 768,
        num_layers: int = 12,
        num_heads: int = 12,
        dropout: float = 0.1,
        use_symbolic: bool = True,
        use_probabilistic: bool = True,
        use_self_supervised: bool = True,
    ):
        super().__init__()
        self.vocab_size = vocab_size
        self.max_seq_length = max_seq_length
        self.hidden_dim = hidden_dim
        self.use_symbolic = use_symbolic
        self.use_probabilistic = use_probabilistic
        self.use_self_supervised = use_self_supervised
        
        # Token and position embeddings
        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)
        self.position_embedding = nn.Parameter(torch.randn(1, max_seq_length, hidden_dim) * 0.02)
        self.dropout = nn.Dropout(dropout)
        
        # Symbolic layer
        if use_symbolic:
            self.symbolic_layer = SymbolicLayer(hidden_dim, hidden_dim)
        
        # Probabilistic layer
        if use_probabilistic:
            self.probabilistic_layer = ProbabilisticLayer(hidden_dim, hidden_dim)
        
        # Self-supervised head
        if use_self_supervised:
            self.self_supervised_head = SelfSupervisedHead(hidden_dim)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Output head
        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        return_self_supervised_loss: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, ...]]:
        """
        Forward pass through the model.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            attention_mask: Attention mask [batch_size, seq_len]
            labels: Target token IDs for language modeling [batch_size, seq_len]
            return_self_supervised_loss: Whether to compute and return self-supervised losses
            
        Returns:
            If not training or not return_self_supervised_loss:
                logits: Output logits [batch_size, seq_len, vocab_size]
            Else:
                (logits, contrastive_loss, predictive_loss): Tuple containing logits and self-supervised losses
        """
        batch_size, seq_len = input_ids.size()
        
        # Get token and position embeddings
        token_embeds = self.token_embedding(input_ids)
        position_embeds = self.position_embedding[:, :seq_len, :]
        x = self.dropout(token_embeds + position_embeds)
        
        # Apply symbolic reasoning if enabled
        if self.use_symbolic:
            x = x + self.symbolic_layer(x)  # Residual connection
        
        # Apply probabilistic modeling if enabled
        if self.use_probabilistic:
            # Get distribution parameters
            mean, var = self.probabilistic_layer(x)
            # Sample from the distribution
            if self.training:
                std = torch.sqrt(var + 1e-8)
                eps = torch.randn_like(std)
                x = mean + eps * std
            else:
                x = mean
        
        # Apply transformer encoder
        if attention_mask is not None:
            # Convert to transformer's attention mask format
            attention_mask = (1.0 - attention_mask) * -10000.0
        
        x = self.encoder(x, src_key_padding_mask=attention_mask)
        
        # Get language model logits
        logits = self.lm_head(x)
        
        # Compute self-supervised losses if enabled and requested
        if self.training and self.use_self_supervised and return_self_supervised_loss:
            contrastive_loss, predictive_loss = self.self_supervised_head(x)
            return logits, contrastive_loss, predictive_loss
        
        return logits
    
    def generate(
        self,
        input_ids: torch.Tensor,
        max_length: int = 100,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.95,
        num_return_sequences: int = 1,
    ) -> torch.Tensor:
        """
        Generate text using the model.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            max_length: Maximum length of generated sequence
            temperature: Sampling temperature (higher = more random)
            top_k: Keep only top k tokens with highest probability
            top_p: Nucleus sampling: keep the top p% of probability mass
            num_return_sequences: Number of sequences to generate per input
            
        Returns:
            Generated token IDs [batch_size * num_return_sequences, max_length]
        """
        self.eval()
        
        if num_return_sequences > 1:
            # Expand inputs for multiple sequences
            input_ids = input_ids.unsqueeze(1).repeat(1, num_return_sequences, 1)
            input_ids = input_ids.view(-1, input_ids.size(-1))
        
        with torch.no_grad():
            for _ in range(max_length - input_ids.size(1)):
                # Get model outputs
                outputs = self(input_ids)
                next_token_logits = outputs[:, -1, :] / temperature
                
                # Apply top-k filtering
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(
                        next_token_logits, top_k, dim=-1
                    )[0][..., -1, None]
                    next_token_logits[indices_to_remove] = -float('inf')
                
                # Apply nucleus sampling
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    # Remove tokens with cumulative probability above the threshold
                    sorted_indices_to_remove = cumulative_probs > top_p
                    # Shift the indices to the right to keep the first token above the threshold
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    # Scatter sorted indices to original indices
                    indices_to_remove = sorted_indices_to_remove.scatter(
                        1, sorted_indices, sorted_indices_to_remove
                    )
                    next_token_logits[indices_to_remove] = -float('inf')
                
                # Sample from the filtered distribution
                probs = F.softmax(next_token_logits, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1)
                
                # Append to the input sequence
                input_ids = torch.cat([input_ids, next_tokens], dim=-1)
                
                # Stop if all sequences have reached the end token
                if (next_tokens == self.config.eos_token_id).all():
                    break
        
        return input_ids
    
    def get_symbolic_expressions(self, input_ids: torch.Tensor) -> dict:
        """
        Extract symbolic expressions from the symbolic layer.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            
        Returns:
            Dictionary mapping token indices to their symbolic expressions
        """
        if not self.use_symbolic:
            return {}
            
        # Get token embeddings
        token_embeds = self.token_embedding(input_ids)
        
        # Get symbolic expressions for each token
        expressions = {}
        for i in range(input_ids.size(1)):  # For each token position
            # Get the symbolic expression for this token
            expr = self.symbolic_layer.get_symbolic_expression(
                token_embeds[:, i, :]
            )
            expressions[i] = str(expr)
            
        return expressions
    
    def get_uncertainty_estimates(self, input_ids: torch.Tensor) -> dict:
        """
        Get uncertainty estimates from the probabilistic layer.
        
        Args:
            input_ids: Input token IDs [batch_size, seq_len]
            
        Returns:
            Dictionary containing mean and variance for each token
        """
        if not self.use_probabilistic:
            return {}
            
        # Get token embeddings
        token_embeds = self.token_embedding(input_ids)
        
        # Get mean and variance from the probabilistic layer
        mean, var = self.probabilistic_layer(token_embeds)
        
        return {
            'mean': mean.detach().cpu().numpy(),
            'variance': var.detach().cpu().numpy(),
            'uncertainty': torch.sqrt(var).detach().cpu().numpy()
        }
